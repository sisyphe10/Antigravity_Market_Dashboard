
import time
import schedule
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import os
import csv
import yfinance as yf
import warnings

import pandas as pd

# Import shared configuration
from config import CATEGORY_MAP, YFINANCE_TICKERS, TARGET_DRAM_ITEMS, TARGET_NAND_ITEMS, CSV_FILE

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
warnings.simplefilter(action='ignore', category=FutureWarning)

# === ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ===
def setup_csv():
    """CSV íŒŒì¼ ì´ˆê¸° ì„¤ì •"""
    if not os.path.exists(CSV_FILE):
        with open(CSV_FILE, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            writer.writerow(['ë‚ ì§œ', 'ì œí’ˆëª…', 'ê°€ê²©', 'ë°ì´í„° íƒ€ì…'])
        print(f"âœ… CSV íŒŒì¼ ìƒì„± ì™„ë£Œ: {CSV_FILE}")
    else:
        print(f"âœ… ê¸°ì¡´ CSV íŒŒì¼ ì‚¬ìš©: {CSV_FILE}")

def setup_driver(headless=True):
    """Selenium ì›¹ë“œë¼ì´ë²„ ì„¤ì •"""
    chrome_options = Options()
    if headless:
        chrome_options.add_argument('--headless')

    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--window-size=1920,1080')
    chrome_options.add_argument('--user-agent=Mozilla/5.0')

    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver

def save_to_csv(data):
    """ì¤‘ë³µ ë°©ì§€ ê¸°ëŠ¥ì´ ê°•í™”ëœ CSV ì €ì¥ (ë°°ì¹˜ ë‚´ ì¤‘ë³µê¹Œì§€ ì œê±°)"""
    try:
        existing_keys = set()
        if os.path.exists(CSV_FILE):
            with open(CSV_FILE, 'r', encoding='utf-8-sig') as f:
                reader = csv.reader(f)
                next(reader, None)
                for row in reader:
                    if len(row) >= 2:
                        key = (row[0], row[1])
                        existing_keys.add(key)

        new_data = []
        current_batch_keys = set()

        for row in data:
            current_key = (row[0], row[1])
            if current_key not in existing_keys and current_key not in current_batch_keys:
                new_data.append(row)
                current_batch_keys.add(current_key)

        if new_data:
            with open(CSV_FILE, 'a', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                writer.writerows(new_data)
            print(f"âœ… {len(new_data)}ê±´ ì €ì¥ ì™„ë£Œ (ì¤‘ë³µ ì œì™¸ë¨)")
            return True
        else:
            print("ğŸ’¡ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ëª¨ë‘ ì¤‘ë³µ)")
            return True

    except Exception as e:
        print(f"\nâŒ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False

def get_last_scfi_date():
    try:
        if not os.path.exists(CSV_FILE): return None
        last_date = None
        with open(CSV_FILE, 'r', encoding='utf-8-sig') as f:
            reader = csv.reader(f)
            next(reader, None)
            for row in reader:
                if len(row) >= 4 and row[3] == 'OCEAN_FREIGHT' and 'SCFI' in row[1]:
                    last_date = row[0]
        return last_date
    except:
        return None

# ==========================================
# 2. [US] ë¯¸êµ­ ì§€ìˆ˜/PER/PBR (yfinance)
# ==========================================
def crawl_us_indices():
    """ë¯¸êµ­ ì§€ìˆ˜ ë° PER/PBR ìˆ˜ì§‘"""
    print(f"\n{'=' * 60}")
    print(f"ğŸ‡ºğŸ‡¸ ë¯¸êµ­ ì§€ìˆ˜/PER/PBR í¬ë¡¤ë§ ì‹œì‘ (yfinance)")
    print(f"{'=' * 60}")

    collected_data = []

    targets = {
        "S&P 500": {"idx": "^GSPC", "etf": "SPY"},
        "NASDAQ": {"idx": "^IXIC", "etf": "QQQ"},
        "RUSSELL 2000": {"idx": "^RUT", "etf": "IWM"}
    }

    for name, tickers in targets.items():
        try:
            # 1. ì§€ìˆ˜ ê°€ê²©
            idx_ticker = yf.Ticker(tickers['idx'])
            hist = idx_ticker.history(period="1d")

            if not hist.empty:
                price = float(hist['Close'].iloc[0])
                d_date = hist.index[0].strftime('%Y-%m-%d')
                collected_data.append((d_date, name, price, 'INDEX_US'))
                print(f"âœ“ {name}: {price:,.2f}")

                # 2. í€ë”ë©˜íƒˆ (ETF ì‚¬ìš©)
                etf_ticker = yf.Ticker(tickers['etf'])
                info = etf_ticker.info

                if 'trailingPE' in info and info['trailingPE']:
                    pe = info['trailingPE']
                    collected_data.append((d_date, f"{name} PER", pe, 'INDEX_US'))

                if 'priceToBook' in info and info['priceToBook']:
                    pbr = info['priceToBook']
                    collected_data.append((d_date, f"{name} PBR", pbr, 'INDEX_US'))

        except Exception as e:
            print(f"âŒ {name} ì˜¤ë¥˜: {e}")

    if collected_data:
        save_to_csv(collected_data)

# ==========================================
# 3. [DRAM/NAND] ë°˜ë„ì²´ ê°€ê²©
# ==========================================
def crawl_dram_nand(data_type):
    """DRAM ë° NAND ê°€ê²© í¬ë¡¤ë§"""
    print(f"\nğŸ“Š {data_type} í¬ë¡¤ë§ ì‹œì‘")
    driver = None
    try:
        driver = setup_driver()
        driver.get(f'https://www.dramexchange.com/#{data_type.lower()}')
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'table')))

        if not driver: return

        current_date = datetime.now().strftime('%Y-%m-%d')
        collected_data = []
        target_items = TARGET_DRAM_ITEMS if data_type == 'DRAM' else TARGET_NAND_ITEMS
        found_items = set()

        tables = driver.find_elements(By.TAG_NAME, 'table')
        for table in tables:
            # 1. í—¤ë”ì—ì„œ 'Average' ì»¬ëŸ¼ ì¸ë±ìŠ¤ ì°¾ê¸°
            price_col_idx = 1  # ê¸°ë³¸ê°’: 2ë²ˆì§¸ ì»¬ëŸ¼ (ì¸ë±ìŠ¤ 1)
            try:
                header_rows = table.find_elements(By.TAG_NAME, 'tr')
                if header_rows:
                    headers = header_rows[0].find_elements(By.TAG_NAME, 'th')
                    if not headers:
                        headers = header_rows[0].find_elements(By.TAG_NAME, 'td')
                    
                    for i, th in enumerate(headers):
                        header_text = th.text.strip().lower()
                        if 'average' in header_text or 'avg' in header_text:
                            price_col_idx = i
                            print(f"  Target column found: '{th.text}' (Index {i})")
                            break
            except Exception as e:
                print(f"  Header parsing warning: {e}")

            # 2. ë°ì´í„° í–‰ íŒŒì‹±
            rows = table.find_elements(By.TAG_NAME, 'tr')
            for row in rows:
                cells = row.find_elements(By.TAG_NAME, 'td')
                if not cells: cells = row.find_elements(By.TAG_NAME, 'th')
                
                # ìµœì†Œí•œ target ì¸ë±ìŠ¤ë³´ë‹¤ëŠ” ê¸¸ì–´ì•¼ í•¨
                if len(cells) <= price_col_idx: continue

                item_name = cells[0].text.strip()
                if item_name in target_items and item_name not in found_items:
                    try:
                        # ì°¾ì€ ì¸ë±ìŠ¤ì˜ ê°€ê²© ì‚¬ìš©
                        price = cells[price_col_idx].text.strip()
                        if price and price.replace('.', '').replace(',', '').isdigit():
                            val = float(price.replace(',', ''))
                            collected_data.append((current_date, item_name, val, data_type))
                            found_items.add(item_name)
                            print(f"âœ“ {item_name}: ${price}")
                    except:
                        pass

        if collected_data:
            save_to_csv(collected_data)
        else:
            print(f"âš ï¸ {data_type} ë°ì´í„° ì—†ìŒ")

    except Exception as e:
        print(f"âŒ {data_type} ì˜¤ë¥˜: {e}")
    finally:
        if driver: driver.quit()

# ==========================================
# 4. [SCFI] í•´ìƒìš´ì„ì§€ìˆ˜
# ==========================================
def crawl_scfi_index():
    print(f"\nğŸš¢ SCFI í¬ë¡¤ë§ ì‹œì‘")
    driver = None
    try:
        driver = setup_driver()
        driver.get('https://en.sse.net.cn/indices/scfinew.jsp')
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'currdate')))

        scfi_date = driver.find_element(By.ID, 'currdate').text.strip()
        scfi_value = None

        tables = driver.find_elements(By.TAG_NAME, 'table')
        for table in tables:
            if 'Comprehensive Index' in table.text:
                rows = table.find_elements(By.TAG_NAME, 'tr')
                for row in rows:
                    if 'Comprehensive Index' in row.text:
                        idx4 = row.find_elements(By.CSS_SELECTOR, 'span.idx4')
                        if idx4: scfi_value = idx4[0].text.strip()

        if scfi_value and scfi_date:
            if get_last_scfi_date() == scfi_date:
                print(f"ğŸ’¡ SCFI ìµœì‹  ìƒíƒœ ({scfi_date})")
            else:
                save_to_csv([(scfi_date, 'SCFI Comprehensive Index', float(scfi_value), 'OCEAN_FREIGHT')])
                print(f"âœ… SCFI ì €ì¥: {scfi_value}")
    except Exception as e:
        print(f"âŒ SCFI ì˜¤ë¥˜: {e}")
    finally:
        if driver: driver.quit()

# ==========================================
# 5. [yfinance] ê¸°íƒ€ ìì‚°
# ==========================================
def crawl_yfinance_data():
    print(f"\nğŸ“ˆ yfinance í¬ë¡¤ë§ ì‹œì‘")
    current_date = datetime.now().strftime('%Y-%m-%d')
    collected_data = []
    for name, info in YFINANCE_TICKERS.items():
        try:
            t = yf.Ticker(info['ticker'])
            h = t.history(period='1d')
            if not h.empty:
                price = float(h['Close'].iloc[0])
                d = h.index[0].strftime('%Y-%m-%d') if info['type'] != 'CRYPTO' else current_date
                collected_data.append((d, name, price, info['type']))
                print(f"âœ“ {name}: {price:.2f}")
        except:
            print(f"âš ï¸ {name} ì‹¤íŒ¨")

    if collected_data: save_to_csv(collected_data)

# ==========================================
# 6. [KR] í•œêµ­ ì§€ìˆ˜ + USD í™˜ì‚°
# ==========================================
def crawl_kr_indices():
    """í•œêµ­ ì§€ìˆ˜(KOSPI/KOSDAQ) + USD í™˜ì‚° ë°ì´í„° ìˆ˜ì§‘"""
    print(f"\n{'=' * 60}")
    print(f"ğŸ‡°ğŸ‡· í•œêµ­ ì§€ìˆ˜ í¬ë¡¤ë§ ì‹œì‘")
    print(f"{'=' * 60}")

    collected_data = []

    try:
        kospi = yf.Ticker('^KS11').history(period='6mo')
        kosdaq = yf.Ticker('^KQ11').history(period='6mo')
        krw = yf.Ticker('KRW=X').history(period='6mo')

        if kospi.empty or kosdaq.empty or krw.empty:
            print("âš ï¸ í•œêµ­ ì§€ìˆ˜ ë°ì´í„° ì—†ìŒ")
            return

        # íƒ€ì„ì¡´ ì œê±°
        kospi.index = kospi.index.tz_localize(None)
        kosdaq.index = kosdaq.index.tz_localize(None)
        krw.index = krw.index.tz_localize(None)

        # KOSPI, KOSDAQ ì›ë³¸ ë°ì´í„°
        for date, row in kospi.iterrows():
            d = date.strftime('%Y-%m-%d')
            collected_data.append((d, 'KOSPI', float(row['Close']), 'INDEX_KR'))

        for date, row in kosdaq.iterrows():
            d = date.strftime('%Y-%m-%d')
            collected_data.append((d, 'KOSDAQ', float(row['Close']), 'INDEX_KR'))

        # USD í™˜ì‚° (KOSPI/USDKRW, KOSDAQ/USDKRW)
        for date in kospi.index:
            if date in krw.index:
                d = date.strftime('%Y-%m-%d')
                fx_rate = float(krw.loc[date, 'Close'])

                kospi_usd = float(kospi.loc[date, 'Close']) / fx_rate
                collected_data.append((d, 'KOSPI/USD', round(kospi_usd, 4), 'INDEX_KR'))

                if date in kosdaq.index:
                    kosdaq_usd = float(kosdaq.loc[date, 'Close']) / fx_rate
                    collected_data.append((d, 'KOSDAQ/USD', round(kosdaq_usd, 4), 'INDEX_KR'))

        print(f"âœ“ KOSPI: {len(kospi)}ì¼, KOSDAQ: {len(kosdaq)}ì¼ ìˆ˜ì§‘")
        print(f"âœ“ KOSPI/USD, KOSDAQ/USD í™˜ì‚° ì™„ë£Œ")

    except Exception as e:
        print(f"âŒ í•œêµ­ ì§€ìˆ˜ ì˜¤ë¥˜: {e}")

    if collected_data:
        save_to_csv(collected_data)

# ==========================================
# Main Execution
# ==========================================
def main():
    print("ğŸš€ ì „ì²´ í¬ë¡¤ë§ ì‹œì‘")
    setup_csv()

    # ìˆœì°¨ì  ì‹¤í–‰
    crawl_dram_nand('DRAM')
    crawl_dram_nand('NAND')
    crawl_scfi_index()
    crawl_yfinance_data()

    crawl_us_indices()
    crawl_kr_indices()

    print(f"\nğŸ“ ê²°ê³¼ íŒŒì¼: {CSV_FILE}")

if __name__ == "__main__":
    main()
